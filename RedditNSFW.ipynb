{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efOSalv5scQt"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d98XKqUDscQ4"
   },
   "outputs": [],
   "source": [
    "def get_posts(subreddit, time_frame, after=None, count=None):\n",
    "    url = \"http://reddit.com/r/\" + subreddit + \"/top.json?t=\" + time_frame + \"&limit=100\"\n",
    "    \n",
    "    if after:\n",
    "        url += \"&after=\" + after\n",
    "    \n",
    "    if count:\n",
    "        url += \"&count=\" + str(count)\n",
    "    \n",
    "    params = {\n",
    "        \"User-Agent\": \"Windows/Python/1.0\"\n",
    "    }\n",
    "\n",
    "#     print(url)\n",
    "    response = requests.get(url, headers=params)\n",
    "    response.raise_for_status()\n",
    "    response_obj = response.json()\n",
    "    \n",
    "#     print(response_obj)\n",
    "    \n",
    "    return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QmjmofUscQ8"
   },
   "outputs": [],
   "source": [
    "def process_posts(response_obj):\n",
    "    data = response_obj['data']\n",
    "    posts = data['children']\n",
    "\n",
    "    processed_posts = []\n",
    "    \n",
    "    for post in posts:\n",
    "        post_data = post['data']\n",
    "        \n",
    "        score = post_data['score']\n",
    "        comments = post_data['num_comments']\n",
    "        \n",
    "        if score < 10 or comments < 10:\n",
    "            continue\n",
    "\n",
    "        title = post_data['title']\n",
    "        text = post_data['selftext']\n",
    "        nsfw = post_data['over_18']\n",
    "\n",
    "        processed_post = {\n",
    "            \"text\": title + \" \" + text,\n",
    "            \"nsfw\": nsfw\n",
    "        }\n",
    "\n",
    "        processed_posts.append(processed_post)\n",
    "    \n",
    "    return processed_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6NKQvYLscRA"
   },
   "outputs": [],
   "source": [
    "def make_bow(processed_posts):\n",
    "    bow = set()\n",
    "    \n",
    "    for post in processed_posts:\n",
    "        text = post[\"text\"]\n",
    "        text = text.lower()\n",
    "        split_text = text.split(\" \")\n",
    "        \n",
    "        bow.update(split_text)\n",
    "    \n",
    "    return bow\n",
    "        \n",
    "    \n",
    "def get_bow_vectors(processed_posts, bow):\n",
    "    feature_vectors = []\n",
    "    target_labels = []\n",
    "    \n",
    "    for post in processed_posts:\n",
    "        post_text = post[\"text\"]\n",
    "        post_label = post[\"nsfw\"]\n",
    "        post_label_encoded = 1 if post_label else 0\n",
    "        \n",
    "        feature_vector = []\n",
    "        \n",
    "        for word in bow:\n",
    "            word_count = post_text.count(word)\n",
    "            feature_vector.append(word_count)\n",
    "        \n",
    "        feature_vectors.append(feature_vector)\n",
    "        target_labels.append(post_label_encoded)\n",
    "    \n",
    "    return feature_vectors, target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "v5Z9mi07scRD",
    "outputId": "40aadf1a-50ab-45d6-ea73-fa48aae5b894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch for subreddit: funny\n",
      "Starting to fetch for subreddit: AskReddit\n",
      "Starting to fetch for subreddit: gaming\n",
      "Starting to fetch for subreddit: pics\n",
      "Starting to fetch for subreddit: science\n",
      "Starting to fetch for subreddit: worldnews\n",
      "Starting to fetch for subreddit: aww\n",
      "Starting to fetch for subreddit: movies\n",
      "Starting to fetch for subreddit: todayilearned\n",
      "Starting to fetch for subreddit: videos\n",
      "Starting to fetch for subreddit: Music\n",
      "Starting to fetch for subreddit: IAmA\n",
      "Starting to fetch for subreddit: news\n",
      "Starting to fetch for subreddit: gifs\n",
      "Starting to fetch for subreddit: EarthPorn\n",
      "Starting to fetch for subreddit: ShowerThoughts\n",
      "Starting to fetch for subreddit: askscience\n",
      "Starting to fetch for subreddit: Jokes\n",
      "Starting to fetch for subreddit: explainlikeimfive\n",
      "Starting to fetch for subreddit: books\n",
      "Starting to fetch for subreddit: food\n",
      "Starting to fetch for subreddit: LifeProTips\n",
      "Starting to fetch for subreddit: DIY\n",
      "Starting to fetch for subreddit: mildlyinteresting\n",
      "Starting to fetch for subreddit: Art\n",
      "Starting to fetch for subreddit: sports\n",
      "Starting to fetch for subreddit: space\n",
      "Starting to fetch for subreddit: gadgets\n",
      "Starting to fetch for subreddit: nottheonion\n",
      "Starting to fetch for subreddit: television\n",
      "Starting to fetch for subreddit: television\n",
      "Starting to fetch for subreddit: photoshopbattles\n",
      "Starting to fetch for subreddit: Documentaries\n",
      "Starting to fetch for subreddit: GetMotivated\n",
      "Starting to fetch for subreddit: listentothis\n",
      "Starting to fetch for subreddit: UpliftingNews\n",
      "Starting to fetch for subreddit: tifu\n",
      "Starting to fetch for subreddit: InternetIsBeautiful\n",
      "warn: Did not find 'after' value in response. Quitting early with only 88 total posts from InternetIsBeautiful\n",
      "Starting to fetch for subreddit: history\n",
      "Starting to fetch for subreddit: Futurology\n",
      "Starting to fetch for subreddit: philosophy\n",
      "Starting to fetch for subreddit: OldSchoolCool\n"
     ]
    }
   ],
   "source": [
    "# according to http://redditlist.com/all\n",
    "# note: excluding r/announcements and r/blog\n",
    "top_20_subreddits = [\"funny\", \"AskReddit\", \"gaming\", \"pics\", \"science\", \n",
    "                     \"worldnews\", \"aww\", \"movies\", \"todayilearned\", \"videos\",\n",
    "                     \"Music\", \"IAmA\", \"news\", \"gifs\", \"EarthPorn\", \"ShowerThoughts\",\n",
    "                     \"askscience\", \"Jokes\", \"explainlikeimfive\", \"books\",\"food\",\"LifeProTips\",\"DIY\",\n",
    "                     \"mildlyinteresting\",\"Art\",\"sports\",\"space\",\"gadgets\",\"nottheonion\",\"television\",\n",
    "                     \"television\",\"photoshopbattles\",\"Documentaries\",\"GetMotivated\",\"listentothis\",\n",
    "                     \"UpliftingNews\",\"tifu\",\"InternetIsBeautiful\",\"history\",\"Futurology\",\"philosophy\",\"OldSchoolCool\"]\n",
    "\n",
    "# top_20_subreddits = [\"askscience\"]\n",
    "# subreddit = \"gonewild\"\n",
    "\n",
    "time_frame = \"year\"\n",
    "\n",
    "posts_to_get_per_subreddit = 500\n",
    "all_processed_posts = []\n",
    "\n",
    "for subreddit in top_20_subreddits:\n",
    "    print(\"Starting to fetch for subreddit: \" + subreddit)\n",
    "    \n",
    "    retrieved_posts = 0\n",
    "    after = None\n",
    "    while retrieved_posts < posts_to_get_per_subreddit:\n",
    "        posts = get_posts(subreddit, time_frame, after, retrieved_posts)\n",
    "\n",
    "        processed_posts = process_posts(posts)  \n",
    "        \n",
    "        if len(processed_posts) == 0:\n",
    "            print(\"warn: Got no posts that met minimum score threshold. Quitting early with only \" \\\n",
    "                   + str(retrieved_posts) + \" total posts from \" + subreddit)\n",
    "            break\n",
    "        \n",
    "        all_processed_posts.extend(processed_posts)\n",
    "        retrieved_posts += len(processed_posts)\n",
    "\n",
    "        after = posts['data']['after']\n",
    "        \n",
    "        if after is None:\n",
    "            print(\"warn: Did not find 'after' value in response. Quitting early with only \" \\\n",
    "                   + str(retrieved_posts) + \" total posts from \" + subreddit)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5P-Gd2OscRH"
   },
   "outputs": [],
   "source": [
    "# check for duplicates - TODO: maybe take set of these?\n",
    "# len([post for post in all_processed_posts if all_processed_posts.count(post) > 1])\n",
    "all_processed_posts = [post for post in all_processed_posts if all_processed_posts.count(post) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JdEp7Ue-scRK",
    "outputId": "194f396d-cbba-4554-85db-8eba6af16e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n",
      "19791\n"
     ]
    }
   ],
   "source": [
    "# check how many nsfw\n",
    "nsfw_posts = [post for post in all_processed_posts if post['nsfw']]\n",
    "print(len(nsfw_posts))\n",
    "print(len(all_processed_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vL8nVDKDNrYF",
    "outputId": "b584b0bf-fb91-491d-8678-4fce1d4817a4"
   },
   "outputs": [],
   "source": [
    "# len(train_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Y8pfxRBscRO"
   },
   "outputs": [],
   "source": [
    "random.shuffle(all_processed_posts)\n",
    "\n",
    "train_end = int(0.8 * len(all_processed_posts))\n",
    "train_set = all_processed_posts[:train_end]\n",
    "test_set = all_processed_posts[train_end:]\n",
    "\n",
    "bow = make_bow(all_processed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnVepCemscRR"
   },
   "outputs": [],
   "source": [
    "train_feature_vectors, train_target_labels = get_bow_vectors(train_set, bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwvB9YF9scRU"
   },
   "outputs": [],
   "source": [
    "test_feature_vectors, test_target_labels = get_bow_vectors(test_set, bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMrjjXnHscRW"
   },
   "outputs": [],
   "source": [
    "# Predict using NB\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(train_feature_vectors, train_target_labels)\n",
    "# test_predictions = nb.predict(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistiv regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(train_feature_vectors, train_target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[133,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_feature_vectors[0]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_LR = logisticRegr.predict(np.array(test_feature_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pXteFw_-scRb",
    "outputId": "f28686d9-a0d1-4f36-cf61-dc3cc719a448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_LR.tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9780247537256883\n",
      "Precision: 0.4791666666666667\n",
      "Recall: 0.27058823529411763\n",
      "F1: 0.3458646616541353\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_target_labels, prediction_LR)\n",
    "precision = precision_score(test_target_labels, prediction_LR)\n",
    "recall = recall_score(test_target_labels, prediction_LR)\n",
    "f1 = f1_score(test_target_labels, prediction_LR)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 10), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "net = MLPClassifier(activation='logistic',hidden_layer_sizes=(100, 10))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "n8RIPldQJdmz",
    "outputId": "2b38f42d-5280-4db1-ed5a-245f65c63706"
   },
   "outputs": [],
   "source": [
    "# # Predict using MLP\n",
    "net = MLPClassifier(activation='logistic',hidden_layer_sizes=(100, 10))\n",
    "print(net)\n",
    "net.fit(train_feature_vectors,train_target_labels)\n",
    "net_test_predictions = net.predict(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xs9kDwEfNcfE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Z6J7OP9GscRZ",
    "outputId": "d5776601-1bcb-422c-cf01-ba2cffc723a2"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_target_labels, test_predictions)\n",
    "precision = precision_score(test_target_labels, test_predictions)\n",
    "recall = recall_score(test_target_labels, test_predictions)\n",
    "f1 = f1_score(test_target_labels, test_predictions)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))\n",
    "\n",
    "# 500 posts per 20 subreddits using full count BoW and MultinominalNB, accuracy = 0.9805352798053528\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "We6Ie9VAscRe",
    "outputId": "049d4078-7da1-4df7-ac6c-6c1ff923b99d"
   },
   "outputs": [],
   "source": [
    "# test_target_labels.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rUM4zAFnJLJa",
    "outputId": "e586211d-d960-4a60-801e-77acb589caa1"
   },
   "outputs": [],
   "source": [
    "# test_target_labels.count(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_feature_vectors,train_target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "RedditNSFW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
