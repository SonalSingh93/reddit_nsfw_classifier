{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efOSalv5scQt"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"15\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"15\"\n",
    "os.environ[\"JOBLIB_START_METHOD\"] = \"forkserver\"\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d98XKqUDscQ4"
   },
   "outputs": [],
   "source": [
    "def get_posts(subreddit, time_frame, after=None, count=None):\n",
    "    url = \"http://reddit.com/r/\" + subreddit + \"/top.json?t=\" + time_frame + \"&limit=100\"\n",
    "    \n",
    "    if after:\n",
    "        url += \"&after=\" + after\n",
    "    \n",
    "    if count:\n",
    "        url += \"&count=\" + str(count)\n",
    "    \n",
    "    params = {\n",
    "        \"User-Agent\": \"Windows/Python/1.0\"\n",
    "    }\n",
    "\n",
    "#     print(url)\n",
    "    response = requests.get(url, headers=params)\n",
    "    response.raise_for_status()\n",
    "    response_obj = response.json()\n",
    "    \n",
    "#     print(response_obj)\n",
    "    \n",
    "    return response_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QmjmofUscQ8"
   },
   "outputs": [],
   "source": [
    "def process_posts(response_obj):\n",
    "    data = response_obj['data']\n",
    "    posts = data['children']\n",
    "\n",
    "    processed_posts = []\n",
    "    \n",
    "    for post in posts:\n",
    "        post_data = post['data']\n",
    "        \n",
    "        score = post_data['score']\n",
    "        comments = post_data['num_comments']\n",
    "        \n",
    "        if score < 10 or comments < 10:\n",
    "            continue\n",
    "\n",
    "        title = post_data['title']\n",
    "        text = post_data['selftext']\n",
    "        nsfw = post_data['over_18']\n",
    "\n",
    "        processed_post = {\n",
    "            \"text\": title + \" \" + text,\n",
    "            \"nsfw\": nsfw\n",
    "        }\n",
    "\n",
    "        processed_posts.append(processed_post)\n",
    "    \n",
    "    return processed_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6NKQvYLscRA"
   },
   "outputs": [],
   "source": [
    "def make_bow(processed_posts):\n",
    "    bow = set()\n",
    "    \n",
    "    for post in processed_posts:\n",
    "        text = post[\"text\"]\n",
    "        text = text.lower()\n",
    "        split_text = text.split(\" \")\n",
    "        \n",
    "        bow.update(split_text)\n",
    "    \n",
    "    return bow\n",
    "        \n",
    "    \n",
    "def get_bow_vectors(processed_posts, bow):\n",
    "    feature_vectors = []\n",
    "    target_labels = []\n",
    "    \n",
    "    for post in processed_posts:\n",
    "        post_text = post[\"text\"]\n",
    "        post_label = post[\"nsfw\"]\n",
    "        post_label_encoded = 1 if post_label else 0\n",
    "        \n",
    "        feature_vector = []\n",
    "        \n",
    "        for word in bow:\n",
    "            word_count = post_text.count(word)\n",
    "            feature_vector.append(word_count)\n",
    "        \n",
    "        feature_vectors.append(feature_vector)\n",
    "        target_labels.append(post_label_encoded)\n",
    "    \n",
    "    return feature_vectors, target_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: NSFW Subreddits below have NSFW names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "v5Z9mi07scRD",
    "outputId": "40aadf1a-50ab-45d6-ea73-fa48aae5b894"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch for subreddit: gonewild\n",
      "warn: Did not find 'after' value in response. Quitting early with only 998 total posts from gonewild\n",
      "Starting to fetch for subreddit: nsfw\n",
      "warn: Did not find 'after' value in response. Quitting early with only 934 total posts from nsfw\n",
      "Starting to fetch for subreddit: realgirls\n",
      "warn: Did not find 'after' value in response. Quitting early with only 998 total posts from realgirls\n",
      "Starting to fetch for subreddit: nsfw_gif\n",
      "warn: Did not find 'after' value in response. Quitting early with only 964 total posts from nsfw_gif\n",
      "Starting to fetch for subreddit: holdthemoan\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 650 total posts from holdthemoan\n",
      "Starting to fetch for subreddit: imgoingtohellforthis\n",
      "warn: Did not find 'after' value in response. Quitting early with only 997 total posts from imgoingtohellforthis\n",
      "Starting to fetch for subreddit: bustypetite\n",
      "warn: Did not find 'after' value in response. Quitting early with only 870 total posts from bustypetite\n",
      "Starting to fetch for subreddit: cumsluts\n",
      "warn: Did not find 'after' value in response. Quitting early with only 837 total posts from cumsluts\n",
      "Starting to fetch for subreddit: legalteens\n",
      "warn: Did not find 'after' value in response. Quitting early with only 884 total posts from legalteens\n",
      "Starting to fetch for subreddit: petitegonewild\n",
      "warn: Did not find 'after' value in response. Quitting early with only 991 total posts from petitegonewild\n",
      "Starting to fetch for subreddit: nsfw_gifs\n",
      "warn: Did not find 'after' value in response. Quitting early with only 749 total posts from nsfw_gifs\n",
      "Starting to fetch for subreddit: adorableporn\n",
      "warn: Did not find 'after' value in response. Quitting early with only 871 total posts from adorableporn\n",
      "Starting to fetch for subreddit: girlsfinishingthejob\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 781 total posts from girlsfinishingthejob\n",
      "Starting to fetch for subreddit: asiansgonewild\n",
      "warn: Did not find 'after' value in response. Quitting early with only 999 total posts from asiansgonewild\n",
      "Starting to fetch for subreddit: rule34\n",
      "warn: Did not find 'after' value in response. Quitting early with only 942 total posts from rule34\n",
      "Starting to fetch for subreddit: amateur\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 877 total posts from amateur\n",
      "Starting to fetch for subreddit: biggerthanyouthought\n",
      "warn: Did not find 'after' value in response. Quitting early with only 859 total posts from biggerthanyouthought\n",
      "Starting to fetch for subreddit: collegesluts\n",
      "warn: Did not find 'after' value in response. Quitting early with only 624 total posts from collegesluts\n",
      "Starting to fetch for subreddit: porninfifteenseconds\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 724 total posts from porninfifteenseconds\n",
      "Starting to fetch for subreddit: tittydrop\n",
      "warn: Did not find 'after' value in response. Quitting early with only 925 total posts from tittydrop\n",
      "Starting to fetch for subreddit: funny\n",
      "warn: Did not find 'after' value in response. Quitting early with only 995 total posts from funny\n",
      "Starting to fetch for subreddit: AskReddit\n",
      "Starting to fetch for subreddit: gaming\n",
      "Starting to fetch for subreddit: pics\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 999 total posts from pics\n",
      "Starting to fetch for subreddit: science\n",
      "warn: Did not find 'after' value in response. Quitting early with only 999 total posts from science\n",
      "Starting to fetch for subreddit: worldnews\n",
      "Starting to fetch for subreddit: aww\n",
      "warn: Did not find 'after' value in response. Quitting early with only 999 total posts from aww\n",
      "Starting to fetch for subreddit: movies\n",
      "Starting to fetch for subreddit: todayilearned\n",
      "Starting to fetch for subreddit: videos\n",
      "Starting to fetch for subreddit: Music\n",
      "warn: Did not find 'after' value in response. Quitting early with only 994 total posts from Music\n",
      "Starting to fetch for subreddit: IAmA\n",
      "warn: Did not find 'after' value in response. Quitting early with only 953 total posts from IAmA\n",
      "Starting to fetch for subreddit: news\n",
      "warn: Did not find 'after' value in response. Quitting early with only 998 total posts from news\n",
      "Starting to fetch for subreddit: gifs\n",
      "warn: Did not find 'after' value in response. Quitting early with only 997 total posts from gifs\n",
      "Starting to fetch for subreddit: EarthPorn\n",
      "Starting to fetch for subreddit: ShowerThoughts\n",
      "Starting to fetch for subreddit: askscience\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 967 total posts from askscience\n",
      "Starting to fetch for subreddit: Jokes\n",
      "Starting to fetch for subreddit: explainlikeimfive\n",
      "warn: Did not find 'after' value in response. Quitting early with only 979 total posts from explainlikeimfive\n",
      "Starting to fetch for subreddit: books\n",
      "warn: Got no posts that met minimum score threshold. Quitting early with only 985 total posts from books\n"
     ]
    }
   ],
   "source": [
    "# according to http://redditlist.com/all\n",
    "# note: excluding r/announcements and r/blog\n",
    "top_40_overall_subreddits = [\"funny\", \"AskReddit\", \"gaming\", \"pics\", \"science\", \n",
    "                     \"worldnews\", \"aww\", \"movies\", \"todayilearned\", \"videos\",\n",
    "                     \"Music\", \"IAmA\", \"news\", \"gifs\", \"EarthPorn\", \"ShowerThoughts\",\n",
    "                     \"askscience\", \"Jokes\", \"explainlikeimfive\", \"books\",\"food\",\"LifeProTips\",\"DIY\",\n",
    "                     \"mildlyinteresting\",\"Art\",\"sports\",\"space\",\"gadgets\",\"nottheonion\",\"television\",\n",
    "                     \"television\",\"photoshopbattles\",\"Documentaries\",\"GetMotivated\",\"listentothis\",\n",
    "                     \"UpliftingNews\",\"tifu\",\"InternetIsBeautiful\",\"history\",\"Futurology\",\"philosophy\",\"OldSchoolCool\"]\n",
    "\n",
    "top_20_sfw_subreddits = [\"funny\", \"AskReddit\", \"gaming\", \"pics\", \"science\", \n",
    "                     \"worldnews\", \"aww\", \"movies\", \"todayilearned\", \"videos\",\n",
    "                     \"Music\", \"IAmA\", \"news\", \"gifs\", \"EarthPorn\", \"ShowerThoughts\",\n",
    "                     \"askscience\", \"Jokes\", \"explainlikeimfive\", \"books\"]\n",
    "\n",
    "# WARNING: nsfw subreddits have nsfw names, as you'd exepct\n",
    "top_20_nsfw_subreddits = [\"gonewild\", \"nsfw\", \"realgirls\", \"nsfw_gif\", \"holdthemoan\",\n",
    "                         \"imgoingtohellforthis\", \"bustypetite\", \"cumsluts\", \"legalteens\",\n",
    "                         \"petitegonewild\", \"nsfw_gifs\", \"adorableporn\", \"girlsfinishingthejob\",\n",
    "                         \"asiansgonewild\", \"rule34\", \"amateur\", \"biggerthanyouthought\", \"collegesluts\",\n",
    "                         \"porninfifteenseconds\", \"tittydrop\"]\n",
    "\n",
    "top_40_combined_subreddits = top_20_nsfw_subreddits\n",
    "top_40_combined_subreddits.extend(top_20_sfw_subreddits)\n",
    "\n",
    "time_frame = \"year\"\n",
    "\n",
    "posts_to_get_per_subreddit = 1000\n",
    "all_processed_posts = []\n",
    "\n",
    "for subreddit in top_40_combined_subreddits:\n",
    "    print(\"Starting to fetch for subreddit: \" + subreddit)\n",
    "    \n",
    "    retrieved_posts = 0\n",
    "    after = None\n",
    "    while retrieved_posts < posts_to_get_per_subreddit:\n",
    "        posts = get_posts(subreddit, time_frame, after, retrieved_posts)\n",
    "\n",
    "        processed_posts = process_posts(posts)  \n",
    "        \n",
    "        if len(processed_posts) == 0:\n",
    "            print(\"warn: Got no posts that met minimum score threshold. Quitting early with only \" \\\n",
    "                   + str(retrieved_posts) + \" total posts from \" + subreddit)\n",
    "            break\n",
    "        \n",
    "        all_processed_posts.extend(processed_posts)\n",
    "        retrieved_posts += len(processed_posts)\n",
    "\n",
    "        after = posts['data']['after']\n",
    "        \n",
    "        if after is None:\n",
    "            print(\"warn: Did not find 'after' value in response. Quitting early with only \" \\\n",
    "                   + str(retrieved_posts) + \" total posts from \" + subreddit)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D5P-Gd2OscRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates - TODO: maybe take set of these?\n",
    "print(len([post for post in all_processed_posts if all_processed_posts.count(post) > 1]))\n",
    "# all_processed_posts = [post for post in all_processed_posts if all_processed_posts.count(post) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JdEp7Ue-scRK",
    "outputId": "194f396d-cbba-4554-85db-8eba6af16e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17801\n",
      "37339\n"
     ]
    }
   ],
   "source": [
    "# check how many nsfw\n",
    "nsfw_posts = [post for post in all_processed_posts if post['nsfw']]\n",
    "print(len(nsfw_posts))\n",
    "print(len(all_processed_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Y8pfxRBscRO"
   },
   "outputs": [],
   "source": [
    "random.shuffle(all_processed_posts)\n",
    "\n",
    "train_end = int(0.8 * len(all_processed_posts))\n",
    "train_set = all_processed_posts[:train_end]\n",
    "test_set = all_processed_posts[train_end:]\n",
    "\n",
    "bow = make_bow(all_processed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnVepCemscRR"
   },
   "outputs": [],
   "source": [
    "train_feature_vectors, train_target_labels = get_bow_vectors(train_set, bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwvB9YF9scRU"
   },
   "outputs": [],
   "source": [
    "test_feature_vectors, test_target_labels = get_bow_vectors(test_set, bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMrjjXnHscRW"
   },
   "outputs": [],
   "source": [
    "# Predict using NB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_feature_vectors, train_target_labels)\n",
    "test_predictions = nb.predict(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8222609161532279\n",
      "Precision: 0.8981581798483207\n",
      "Recall: 0.7041336353340883\n",
      "F1: 0.7893985081733059\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_target_labels, test_predictions)\n",
    "precision = precision_score(test_target_labels, test_predictions)\n",
    "recall = recall_score(test_target_labels, test_predictions)\n",
    "f1 = f1_score(test_target_labels, test_predictions)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))\n",
    "\n",
    "# Top 20 NSFW + Top 20 SFW, 1000 posts each, full count BoW with NB:\n",
    "#Accuracy: 0.8222609161532279\n",
    "#Precision: 0.8981581798483207\n",
    "#Recall: 0.7041336353340883\n",
    "#F1: 0.7893985081733059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(solver=\"liblinear\", max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(train_feature_vectors, train_target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predictions = logisticRegr.predict(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.899370226450489\n",
      "Precision: 0.8677929739876643\n",
      "Recall: 0.9261591299370349\n",
      "F1: 0.8960265817527344\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_target_labels, lr_predictions)\n",
    "precision = precision_score(test_target_labels, lr_predictions)\n",
    "recall = recall_score(test_target_labels, lr_predictions)\n",
    "f1 = f1_score(test_target_labels, lr_predictions)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))\n",
    "\n",
    "# Top 20 NSFW + Top 20 SFW, 1000 posts each, full count BoW with LR:\n",
    "# Accuracy: 0.899370226450489\n",
    "# Precision: 0.8677929739876643\n",
    "# Recall: 0.9261591299370349\n",
    "# F1: 0.8960265817527344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davetand/.local/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=2000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(train_feature_vectors,train_target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm.predict(test_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8926705078386707\n",
      "Precision: 0.8743397275507367\n",
      "Recall: 0.9001144819690898\n",
      "F1: 0.8870399097447468\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_target_labels, svm_predictions)\n",
    "precision = precision_score(test_target_labels, svm_predictions)\n",
    "recall = recall_score(test_target_labels, svm_predictions)\n",
    "f1 = f1_score(test_target_labels, svm_predictions)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))\n",
    "\n",
    "# Top 20 NSFW + Top 20 SFW, 1000 posts each, full count BoW with LinearSVM (did not converge):\n",
    "# Accuracy: 0.8926705078386707\n",
    "# Precision: 0.8743397275507367\n",
    "# Recall: 0.9001144819690898\n",
    "# F1: 0.8870399097447468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word embeddings data into w\n",
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlen = w[\"word\"].size\n",
    "# Average word embeddings\n",
    "def getAverageWordEmbeddings(review):\n",
    "    length_of_review = len(review)\n",
    "    result = [w[word] for word in review if word in w]\n",
    "#     print(np.sum(result, axis=0)/len(result))\n",
    "    return np.sum(result, axis=0)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Find the word emebeddings for the given posts\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "word2vec_train_data = []\n",
    "word2vec_train_labels = []\n",
    "word2vec_test_data = []\n",
    "word2vec_test_labels = []\n",
    "\n",
    "\n",
    "for post in train_set:\n",
    "    avg_word_embedding = getAverageWordEmbeddings(post['text'])\n",
    "    if avg_word_embedding.size==1:\n",
    "        continue\n",
    "#     if avg_word_embedding.size!=300:\n",
    "#         print(avg_word_embedding.size)\n",
    "    word2vec_train_data.append(avg_word_embedding)\n",
    "    word2vec_train_labels.append(post['nsfw'])\n",
    "word2vec_train_data = np.array(word2vec_train_data)\n",
    "word2vec_train_labels = np.array(word2vec_train_labels)\n",
    "\n",
    "for post in test_set:\n",
    "    avg_word_embedding = getAverageWordEmbeddings(post['text'])\n",
    "    if avg_word_embedding.size==1:\n",
    "        continue\n",
    "#     print(avg_word_embedding.size)\n",
    "    word2vec_test_data.append(avg_word_embedding)\n",
    "    word2vec_test_labels.append(post['nsfw'])\n",
    "word2vec_test_data = np.array(word2vec_test_data)\n",
    "word2vec_test_labels = np.array(word2vec_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 50, 10), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "net = MLPClassifier(activation='relu',hidden_layer_sizes=(100, 50, 10),max_iter=300)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "n8RIPldQJdmz",
    "outputId": "2b38f42d-5280-4db1-ed5a-245f65c63706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 50, 10), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Predict using MLP\n",
    "net.fit(word2vec_train_data,word2vec_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xs9kDwEfNcfE"
   },
   "outputs": [],
   "source": [
    "net_test_predictions = net.predict(word2vec_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Z6J7OP9GscRZ",
    "outputId": "d5776601-1bcb-422c-cf01-ba2cffc723a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7842006437768241\n",
      "Precision: 0.7845516424977804\n",
      "Recall: 0.7505662514156285\n",
      "F1: 0.7671827521342787\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(word2vec_test_labels, net_test_predictions)\n",
    "precision = precision_score(word2vec_test_labels, net_test_predictions)\n",
    "recall = recall_score(word2vec_test_labels, net_test_predictions)\n",
    "f1 = f1_score(word2vec_test_labels, net_test_predictions)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1: \" + str(f1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Results:\n",
    "hidden layer : 100, logistic activation function\n",
    "Accuracy: 0.777494635193133\n",
    "Precision: 0.7803052978150254\n",
    "Recall: 0.7381087202718006\n",
    "F1: 0.7586206896551724\n",
    "  \n",
    "50 as first input layer, logistic activation function\n",
    "Accuracy: 0.7950643776824035\n",
    "Precision: 0.7825719120135364\n",
    "Recall: 0.785673839184598\n",
    "F1: 0.7841198078553263\n",
    "\n",
    "hidden_layer_sizes=(100, 50, 10),max_iter=300\n",
    "Accuracy: 0.7964055793991416\n",
    "Precision: 0.7718682505399568\n",
    "Recall: 0.8094563986409966\n",
    "F1: 0.7902155887230514\n",
    "\n",
    "activation='relu',hidden_layer_sizes=(100, 50, 10),max_iter=300\n",
    "Accuracy: 0.7842006437768241\n",
    "Precision: 0.7845516424977804\n",
    "Recall: 0.7505662514156285\n",
    "F1: 0.7671827521342787"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "RedditNSFW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
